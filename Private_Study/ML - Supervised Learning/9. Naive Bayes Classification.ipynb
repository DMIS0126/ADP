{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 나이브베이즈(Naive Bayes) 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 나이브베이즈는 데이터가 각 클래스에 속할 확률을 계산하는 조건부 확률 기반의 분류 방법이다.\n",
    "* 베이즈는 베이지안 통계를 기반으로 입력특징이 클래스 ㅈ번체의 확률 분포 대비 특정 클래스에 속할 확률을 정리하였다는 점에서 비롯되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 베이즈 정리(Bayes' Theorem)\n",
    "* 두 확률변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리\n",
    "* 두 사건 A, B에 대하여 사건 A가 일어난 것을 전제로 한 사건 B의 조건부 확률(**$\\mathrm{P}(B \\mid A)$**)과 B의 확률(**$\\mathrm{P}(B)$**)만을 알고 있을 때, 사건 B가 일어난 것을 전제로 한 사건 A의 조건부 확률(**$\\mathrm{P}(A\\mid B)$**)을 구하는 것이다.\n",
    "\n",
    "* 가정\n",
    "    * 서로소인 $n$개의 집합 $A_1,\\ A_2,\\ \\cdots,\\ A_n$에 대하여 표본공간 S는 $S=\\displaystyle\\cup_{i=1}^{n}A_{i}$이다.\n",
    "    * 사건 B는 표본공간 S 위에서 정의되었고, $\\mathrm{P}(A)\\ne0$이다.\n",
    "\n",
    "* 내용\n",
    "    $\\begin{aligned}\\mathrm{P}(A_k\\mid B)&=\\dfrac{\\mathrm{P}(A_k \\cap B)}{\\mathrm{P}(B)}\\\\&=\\dfrac{\\mathrm{P}(A_k\\cap B)}{\\mathrm{P}(A_1\\cap B)+\\mathrm{P}(A_2\\cap B)+\\cdots+\\mathrm{P}(A_n\\cap B)}\\\\&=\\dfrac{\\mathrm{P}(B\\mid A_k)\\mathrm{P}(A_k)}{\\mathrm{P}(B\\mid A_1)\\mathrm{P}(A_1)+\\mathrm{P}(B\\mid A_2)\\mathrm{P}(A_2)+\\cdots+\\mathrm{P}(B\\mid A_n)\\mathrm{P}(A_n)}  \\end{aligned}$\n",
    "\n",
    "    * $\\mathrm{P}(A\\mid B)$(사후확률) : 사건 B가 발생했을 때, 사건 A가 발생할 확률\n",
    "    * $\\mathrm{P}(B\\mid A)$(우도(Likelihood)) : 사건 A가 발생했을 때, 사건 B가 발생할 확률\n",
    "    * $\\mathrm{P}(A\\cap B)$ : 사건 A와 B가 동시에 발생할 확률\n",
    "    * $\\mathrm{P}(A)$(사전확률) : 사건 A가 발생할 확률\n",
    "    * $\\mathrm{P}(B)$(관찰값) : 사건 B가 발생할 확률\n",
    "\n",
    "> ✅ 예시\n",
    ">\n",
    "> 자동차 사고로 사망한 사람의 40%는 안전띠를 매지 않았다고 한다. 그렇다면 60%는 안전띠를 매고 죽었다는 의미인데 안전띠가 더 위험한 것인가?\n",
    ">\n",
    "> * 이때, 전체 운전자 중에서 안전띠를 맨 사람의 비율이 주어진다면 위의 논리를 쉽게 반박할 수 있다.\n",
    ">\n",
    ">   * 만약 전체 운전자의 95%가 안전띠를 맸고, 5%는 안전띠를 매지 않았다고 하고, 전체 운전자 1만 명 중 1명 꼴로 자동차 사고로 사망한다고 가정하자.\n",
    ">\n",
    ">   * 이때, 위의 예시에서 알 수 있는 것은 주장하는 것은 안전띠를 매는 사건을 A, 사망하는 사건을 B라 할 때 $\\mathrm{P}(A\\mid B) = 0.6$이라는 것이므로 안전띠가 더 위험한 것인지 확인하기 위해서는\n",
    ">\n",
    ">       * 안전띠를 맸을 때, 사망할 확률과 안전띠를 맸을 때, 사망하지 않을 확률인 $\\mathrm{P}(B\\mid A),\\ \\mathrm{P}(B\\mid A^{C})$를 살펴봐야 한다.\n",
    ">\n",
    ">       * $\\mathrm{P}(B\\mid A)=\\dfrac{\\mathrm{P}(B\\cap A)}{\\mathrm{P}(A)}=\\dfrac{\\mathrm{P}(A\\mid B)\\mathrm{P}(B)}{\\mathrm{P}(A)}=\\dfrac{0.6 \\times 0.0001}{0.95} = 0.000063$\n",
    ">\n",
    ">           * 여기서 $\\mathrm{P}(B \\cap A)=\\mathrm{P}(B)\\mathrm{P}(A)$와 같이 계산하지 않은 이유는 두 사건이 독립인지 알 수 없기 때문이다.\n",
    ">\n",
    ">       * $\\mathrm{P}(B\\mid A^C)=\\dfrac{\\mathrm{P}(B\\cap A^C)}{\\mathrm{P}(A^C)}=\\dfrac{\\mathrm{P}(A^C\\mid B)\\mathrm{P}(B)}{\\mathrm{P}(A^C)}=\\dfrac{0.4 \\times 0.0001}{0.05} = 0.0008$\n",
    ">\n",
    ">   * 위의 결과로 미루어봤을 때, 자동차 사고로 사망할 확률은 안전띠를 맸을 때보다 안전띠를 매지 않았을 때가 10배 이상 크다는 것을 알 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 나이브베이즈 분류\n",
    "* 하나의 속성값을 기준으로 다른 속성이 독립적이라 전제했을 때(클래스 조건 독립성), 해당 속성값이 클래스 분류에 미치는 영향을 측정하는 것\n",
    "> ✅ 예시\n",
    ">\n",
    "> 메일함으로 전송되는 전체 메일 중 10%가 스팸이고, 전체 메일 중 4%가 복권이라는 단어를 갖고 있다고 가정해보자.\n",
    ">\n",
    "> * 특정 메일이 스팸메일인 사건을 A, 복권이라는 단어가 들어간 사건을 B라 하면 스팸메일이면서 복권이라는 단어가 들어간 사건은 $A\\cap B$와 같이 나타낼 수 있다.\n",
    ">\n",
    "> * 이때, 두 사건 A, B가 독립이라면 $\\mathrm{P}(A\\cap B)=\\mathrm{P}(A)\\mathrm{P}(B)$와 같이 계산할 수 있다.\n",
    "\n",
    "* 위의 예시에서 나이브베이즈 분류의 작동원리를 생각하면 다음과 같다.\n",
    "    * 나이브베이즈의 목표는 사후확률을 계산하는 것이므로\n",
    "        * 메일에서 복권이라는 단어가 발견되었을 때, 이 메일이 스팸일 확률을 구하는 것이다.\n",
    "\n",
    "* 장점\n",
    "    * 간단하고 빠르다.\n",
    "    * 노이즈와 결측치에 강하다.\n",
    "    * 예측을 위한 추정확률을 쉽게 얻을 수 있다.\n",
    "\n",
    "* 단점\n",
    "    * 모든 특징이 동등하게 중요하며 독립이라고 가정하기 때문에, 가정이 잘못된 경우들이 종종 있다.\n",
    "    * 가정된 확률이 예측된 클래스보다 덜 신뢰할 만하다.\n",
    "\n",
    "\n",
    "* 따라서, 나이브베이즈 분류는 데이터가 많지 않거나, 추정의 목적이 미래 예측일 때, 사용한다.\n",
    "\n",
    "#### 나이브베이즈 분류기(scikit-learn)\n",
    "* BernoulliNB\n",
    "    * 이진데이터에 적용한다.\n",
    "    * 예를 들어, 주사위를 4번 던져 1이 1번, 3이 2번, 4가 1번 나왔다고 가정할 때, 1부터 6까지의 숫자 중 나온 숫자에 대한 결과를 [1, 0, 1, 1, 0, 0]과 같이 나타낼 수 있다.\n",
    "    * 실제 사용 예시 : 스팸메일 분류\n",
    "* MultinomialNB\n",
    "    * 이산형 데이터에 적용한다.\n",
    "    * 예를 들어, 주사위를 4번 던져 1이 1번, 3이 2번, 4가 1번 나왔다고 가정할 때, 1부터 6까지의 숫자 중 나온 숫자의 횟수 대한 결과를 [1, 0, 2, 1, 0, 0]과 같이 나타낼 수 있다.\n",
    "    * 실제 사용 예시 : 영화 감상평을 토대로 긍정작/부정적 리뷰 분류\n",
    "* GaussianNB\n",
    "    * 연속형 데이터에 적용한다.\n",
    "    * 특징들의 값이 정규분포(가우시안 분포)라는 가정 하에 조건부 확률을 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BernoulliNB 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n",
      "(3900, 1) (1672, 1) (3900,) (1672,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# # chardet을 이용하여 오류 해결\n",
    "# !pip install chardet\n",
    "# import chardet\n",
    "# with open('./data/spam.csv', 'rb') as rawdata:\n",
    "#     result = chardet.detect(rawdata.read(10000))\n",
    "#\n",
    "# print(result)\n",
    "\n",
    "spam = pd.read_csv('./data/spam.csv', encoding = 'utf-16')\n",
    "spam.info()\n",
    "\n",
    "# Unnamed:2 ~ 4는 결측치가 전체의 50%를 넘어가므로 제거\n",
    "spam = spam[['v1', 'v2']]\n",
    "spam.head()\n",
    "spam.v1.unique()\n",
    "\n",
    "# v1 변수는 해당 메일이 스팸인지 아닌지이므로 spam이면 1, ham이면 0으로 라벨링\n",
    "spam.v1 = spam.v1.map({'spam' : 1, 'ham' : 0})\n",
    "spam\n",
    "\n",
    "# v2 변수를 설명변수로, v1 변수를 타깃변수로 지정하고 7:3의 비율로 train, test 데이터 분리\n",
    "    # 이때, 타깃변수의 클래스 비율이 반영되도록 stratify 사용\n",
    "y = spam.pop('v1')\n",
    "x = spam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=731, stratify=y)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "x_train = x_train.loc[:]['v2']\n",
    "x_test = x_test.loc[:]['v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3900, 7193)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer는 시리즈를 넣어야 제대로 작동한다.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary=True)\n",
    "x_traincv = cv.fit_transform(x_train)\n",
    "print(x_traincv.shape)\n",
    "\n",
    "encoded_input = x_traincv.toarray()\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['just', 'off', 'others', 'the', 'took', 'wondering'], dtype='<U32')]\n",
      "Accuracy Score :  0.972488038277512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1448\n",
      "           1       0.99      0.80      0.89       224\n",
      "\n",
      "    accuracy                           0.97      1672\n",
      "   macro avg       0.98      0.90      0.94      1672\n",
      "weighted avg       0.97      0.97      0.97      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 역으로 인코딩된 이메일 제목에 어떤 단어들이 포함되어 있는지 확인하려면, inverse_transform을 사용하면 된다.\n",
    "print(cv.inverse_transform(encoded_input[[0]]))\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(x_traincv, y_train)\n",
    "\n",
    "x_testcv = cv.transform(x_test)\n",
    "\n",
    "pred = bnb.predict(x_testcv)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, pred)\n",
    "print('Accuracy Score : ', acc)\n",
    "\n",
    "# report 확인\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MultinomialNB 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# keras.datasets 중 imdb 이용\n",
    "from keras.datasets import imdb\n",
    "(x_train, y_train), (x_test, y_test) = imdb.loac_data()\n",
    "\n",
    "# 다운로드가 오래걸려서 실패"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GaussianNB 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
