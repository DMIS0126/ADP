# 밑바닥부터 시작하는 통계분석 (5) 선형 회귀분석


> ##### 2023년 6월 3일 토요일 ADP 실기 시험을 위한 공부(5/16~6/2)
##### 🔥 기본적인 데이터 전처리 및 머신러닝 파트는 주피터 노트북을 이용하여 공부하고 [깃허브](https://github.com/DMIS0126/ADP/tree/main/Private_Study)에 정리 완료
##### 🔥 통계분석은 이론 정리가 되어야 코드를 제대로 적을 수 있을 것 같다고 판단하여 블로그에 정리 시작

---

## ✐ 지금까지 배운 내용 정리
* 머신러닝은 예측의 성공 확률을 높이는 데에 목적이 있다면, 
* 전통적 통계분석 방법은 정해진 분포나 가정을 통해 실패 확률을 줄이고 원인을 찾는 데에 목적이 있다.
* 이미 머신러닝 파트에서 선형 회귀, 다항 회귀를 학습하였으므로 이 절에서는 전통적 통계기법을 사용한 회귀분석이 머신러닝보다 설명력이 좋은지를 비교해보고자 한다.

   🗒 지금까지 배운 분석들을 각각 언제 사용하는지 표로 정리
  ![](https://velog.velcdn.com/images/dmis/post/a26bbdce-68a5-4d10-adfd-fff2097e487b/image.png)

  > * ADP 문제를 풀 때 가장 먼저 확인해야 하는 것은 데이터의 타입이다.
	  > 
  > * 데이터의 타입에 따라 분석기법이 달라지며, 종속변수와 독립변수가 모두 연속형일 때는 주로 회귀분석을 사용한다.


---

## ✐ 회귀분석

### ⚑ 개념
* 회귀분석은 머신러닝과 다르게 식으로 표현하므로 해석력을 높일 수 있다.
* 또한, 변수들 사이의 상관관계를 밝히고 모형을 적합하여 관심있는 변수를 예측하거나 추론하기 위해 사용한다.
* 독립변수의 개수가 하나일 때, 단순 선형 회귀분석이라 하며, 독립변수가의 개수가 두 개 이상이면 다중 선형 회귀분석이라 한다.

### ⚑ 분석에 대한 평가
#### 1. 단순 선형 회귀분석
  * 머신러닝의 회귀분석과 마찬가지로 잔차의 합이 최소가 되는 최소제곱법을 사용한다.
    * 즉, 하나의 선이 전체 데이터를 얼마나 잘 설명할 수 있는지가 회귀분석의 평가지표가 된다.
  * 주로 사용하는 평가지표는 $R^{2}$(결정계수)와 RMSE이다.
  > 💡 $R^{2}$(결정계수)
  >
  > 결정계수를 이해하기 위해 필요한 것
  > 
  > * SST(total sum of squares)(총변동) : 개별 y의 편차 제곱의 합
  >   * $SST = \displaystyle \sum_{i=1}^n(y_i-\overline{y})^{2}$
  > 
  > * SSE(explained sum of squares)(설명된 변동) : 회귀식 추정 y의 편차 제곱의 합
  >   * $SSE = \displaystyle \sum_{i=1}^n(\hat{y}_i-\overline{y})^{2}$
  > 
  > * SSR(residual sum of squares)(설명되지 않은 변동) : 잔차(residual = $y_i-\hat{y}_i$)의 제곱의 합
  >   * $SSR = \displaystyle \sum_{i=1}^n(y_i-\hat{y})^{2}$
  > 
  > $\therefore SST = SSE+SSR$
  > 
  > 이때, $R^{2}$을 다음과 같이 정의한다.
  > 
  > $R^{2}=1-\dfrac{SSR}{SST}$
  >  * 총 변동 중 설명된 변동의 비율
  >
  > * 즉, $R^{2}$은 <span style="color:#7FFFD4;">회귀 추정선이 전체 데이터를 얼마나 잘 설명하고 있는가</span>를 의미하므로 값이 높다면, 구한 <span style="color:#7FFFD4;">회귀 추정 직선으로 새로운 값을 예측하거나 추정하여도 믿을 만하다</span>는 것을 의미한다.

  > 💡 RMSE(Root Mean Square Error)(평균 제곱근 오차)
  > 
  > $RMSE = \sqrt{\displaystyle\sum_{i=1}^{n}\dfrac{(\hat{y}_i-y_i)^{2}}{n-2}}$
  >  * $n-2$ : 자유도
  >
  > * RMSE 또한 선형 모델이 예측한 값과 실제 관측값의 차이를 의미한다는 개념이 중요하다.
  > 
  > * 모델의 성능을 테스트해봤을 때, 이 값이 작다면 예측을 잘했다고 할 수 있다.
  
#### 🧑‍💻 단순 선형 회귀분석 실습
```python
# kc_house_data의 sqft_living을 독립변수, price를 종속변수로 설정하여 단순 선형 회귀분석 해보고, 결과 해석하기
import pandas as pd
house = pd.read_csv('./data/kc_house_data.csv')
house = house[['sqft_living', 'price']]
house.corr()

from statsmodels.formula.api import ols # OLS: Ordinary Least Squares
import matplotlib.pyplot as plt

x = house['sqft_living']
y = house['price']

lr = ols('price ~ sqft_living', data = house).fit()
y_pred = lr.predict(x)

plt.scatter(x, y)
plt.plot(x, y_pred, color = 'red') # 회귀직선 추가
plt.xlabel('sqft_living')
plt.ylabel('price')
plt.title('Linear Regression Result')
plt.show()
```
![](https://velog.velcdn.com/images/dmis/post/6fd0a41d-aa63-432e-8162-664685561791/image.png)
> 👀  **위의 차트를 통한 <span style="color:#7FFFD4;">모형이 데이터를 잘 적합하고 있는가?</span>에 대한 해석 **
> 
> * 시각화 결과 직관적으로도 직선이 모든 데이터를 잘 설명하지는 못하고 있는 것으로 보인다.
> 
> * 또한, (0, 0)에서 멀어질수록 오차의 분산이 커지는 특정한 패턴을 이루고 있으므로 단순 회귀분석으로는 데이터를 충분히 설명할 수 없는 것으로 보인다.

``` python
# 단순 선형 회귀분석의 요약 정보 확인
lr.summary()
```
![](https://velog.velcdn.com/images/dmis/post/a48c7c3d-e14a-4e0f-a3d4-e38950e4b388/image.png)

> 👀 ** 위의 요약 정보에 대한 해석** 
>
> **<span style="color:#7FFFD4;">1. 회귀 모형이 통계적으로 유의한가?</span>**
> * 귀무가설 : 회귀 모형은 통계적으로 유의하지 않다.
> * 대립가설 : 회귀 모형은 통계적으로 유의하다.
> * 위의 요약 정보에서 F통계량과 p-value인 Prob(F-statistic)을 확인할 수 있는데, p-value가 0이므로 유의수준 0.05 하에 귀무가설을 기각하여 대립가설을 채택한다.
>   * 즉, 회귀 모형은 통계적으로 유의하다.
> ---
> **<span style="color:#7FFFD4;">2. 모형은 데이터를 얼마나 설명할 수 있는가?</span>**
>  $R^{2}=0.493$으로 이 모형이 전체 데이터의 49.3%를 설명한다고 할 수 있다.
> * 💡 통계 모델이 유의하다고 하여, 모델의 성능이 좋은 것은 아니다.
>   * 하지만, $R^{2}=0.493$인 것이 산업에 따라서는 엄청난 정확도를 보이는 것으로 판달할 수도 있다.
> 
> ---
> **<span style="color:#7FFFD4;">3. 모형 내의 회귀 계수는 유의한가?</span>**
> * intercept는 모형의 상수값이므로 관심사가 아니다.
> * 변수인 sqft_living의 회귀계수에 대한 p-value인 P>|t|를 보면, 0이므로 유의수준 0.05 하에 귀무가설을 기각하여 대립가설을 채택한다.
>   * 즉, sqft_living은 통계적으로 유의미한 변수로 볼 수 있다.
> 
> ---
>  위의 결과를 모두 종합하여, 전체 데이터의 49.3%를 설명할 수 있는 다음과 같은 회귀식을 도출할 수 있다.
> * ${Price} = {sqft\_livng} \times 280.6236 + (-43580.743094)$
 
--- 
 
 
#### 2. 다중 선형 회귀분석
* 다중 선형 회귀분석은 독립변수의 개수가 2 이상일 때 사용하는데, 독립변수의 개수가 늘어나면 모델의 $R^{2}$이 증가하게 된다.
* 이에 따라 모델의 성능이 독립변수의 개수에 따라 증가하는 것에 대한 패널티를 적용시킨다.
  $AdjustedR^{2}=\displaystyle1-\dfrac{SSR\div(n-k-1)}{SST\div(n-1)}$
  > 💡 n이 커진다면 1, 2들의 값의 차이는 큰 의미가 없어 자유도로 나누지 않고 n으로 나눈다.
  > 
  >   sklearn.metirics에 있는 mean_square_error도 n으로 나눈 값이다.
  
### ⚑ 회귀분석 검토사항
1. 모델이 데이터를 잘 적합하고 있는가?
   * 모형의 잔차(residual= $y_i-\hat{y}_i$)가 특정 패턴을 이루고 있지 않아야 한다.
   ![](https://cdn.teamturing.com/cms/1684451284_residual.png)
2. 회귀 모형이 통계적으로 유의한가?
   * 선형 회귀 모형의 통계량은 F 통계량을 사용한다. F 통계량의 p-value가 유의수준보다 작으면 회귀식이 통계적으로 유의하다고 볼 수 있다. 이때의 귀무가설과 대립가설은 다음과 같다.
     * 귀무가설(H0) : 회귀 모형은 유의하지 않다.
     * 대립가설(H1) : 회귀 모형은 유의하다.
3. 모형은 데이터를 얼마나 설명할 수 있는가?
   * $R^{2}$의 값을 확인한다. $R^{2}$은 비율이기에 0~1의 값을 가지며, 추정된 회귀식이 전체 데이터에서 설명할 수 있는 데이터의 비율이다.
4. 모형 내의 회귀계수는 유의한가?
   * 회귀계수에 대해서는 각 독립변수를 검정해야 한다.
   * 회귀계수에 대한 통계량은 t값이며, p-value가 유의수준보다 작으면 회귀계수가 통계적으로 유의하다고 할 수 있다.


### ⚑ 다중 회귀분석 시 유의사항
#### 1. 다중공선성
* 다중공선성이란 회귀분석에서 독립변수들 간 강한 상관관계가 나타나는 문제를 말한다.
* 다중공선성의 문제가 존재하면 정확한 회귀계수 추정이 어렵고, 회귀분석에서는 독립변수의 수가 증가할수록 모델의 정확도가 올라가는 문제가 발생하므로 다중공선선이 존재한다면 하나의 변수를 제거해주거나 해당 변수에 패널티를 주어 모델에 미치는 영향력을 줄여야 한다.
  * 패널티를 줄이는 방법은 Ridge, Lasso 회귀분석에서 다룬다.)
* 다중공선성을 검사하고 진단하는 방법은 다음과 같다.
  1. 독립변수들 간의 상관계수를 구하여 상관성을 직접 파악, 절댓값이 0.9 이상이라면 다중공선성이 있다고 판단한다.
  2.  다중공선성이 의심되는 두 독립변수만으로 이루어진 회귀분석으로 허용 오차를 구했을 때, 0.1 이하이면 다중공선성 문제가 심각하다고 할 수 있다.
    > 💡 허용 오차  $=1-R^2$
  3. VIF(variance inflation factor)의 값이 10 이상이라면 다중공선성이 존재할 것으로 예상한다.
    > 💡 $VIF = \dfrac{1}{1-R^2}$
* 2, 3번 방법은 결정계수에 의해 결정되는데, 독립뼌수의 개수가 많아질 수록 계산의 양은 ${}_{n}\mathrm{C}_{2}$로 많아진다.
* VIF는 파이썬을 통해 구할 수 있기 때문에, VIF로 최대한 진단하고 2, 3번을 이용하는 것이 좋다.

#### 2. 변수선택법
* 다중공선성이 존재하는 경우 하나의 변수를 삭제하거나 패널티를 주어 해결하는데, 다중 선형 회귀분석에서는 이 외에도 변수를 제거해야 하는 경우가 생긴다.
  * 모형 내 설명변수의 수가 증가할수록 모델에 영향을 미치는 데이터를 관리하는 데 많은 비용과 노력이 요구된다.
* 따라서 종속변수에 영향을 미치는 유의미한 독립변수만을 선택하여 최적의 회귀방정식을 도출하는 과정이 필요하다.
  * 변수를 선택할 때에는 모델의 유의성 판단의 근거로 삼았던 F통계량이나 AIC와 같은 기준값을 근거로 제거 또는 선택한다.
  > 💡 $AIC = -2\ln(L)+2k$
  >
  > $2\ln(L)$ : 모형의 적합도
  >
  >   * 이에 따라 AIC의 값이 낮다는 것은 모형의 적합도가 높은 것을 의미한다.
  >
  > $k$ = 추정된 파라미터의 개수
  
  
* 변수 선택법에는 다음과 같이 3가지 방법이 있다.

  |구분|내용|
  |:--:|--|
  |전진선택법|단순 선형 회귀분석에서 하나의 변수씩 추가해가며 모델의 정확도를 높이는 방법|
  |후진제거법|모든 변수를 추가한 상태에서 유의하지 않은 변수들을 제거해가며 모델의 성능을 높이는 방법|
  |단계적 선택법|변수를 추가, 제거하며 모델의 성능을 높이는 방법|
  
#### 🧑‍💻 다중 선형 회귀분석 실습

```python
# cars93 데이터로 해보기
import pandas as pd

cars = pd.read_csv('./data/cars93.csv')
cars.drop(columns = 'Unnamed: 0', inplace=True)
cars.info()

# price를 종속변수, enginesize, rpm, weight, length, mpg.city, mpg.highway를 독립변수로 놓고 price에 영향을 주는 변수를 찾기 위한 회귀분석 해보기
import statsmodels.api as sm
import statsmodels.formula.api as smf

# ols 모델의 formula를 정의할 때, 일부 특수문자는 쓸 수 없기에 데이터 전처리
cars.columns = cars.columns.str.replace('.', '')
model = smf.ols(formula = 'Price ~ EngineSize + RPM + Weight + Length + MPGcity + MPGhighway', data = cars)
result = model.fit()
result.summary()
```
![](https://velog.velcdn.com/images/dmis/post/3b07d224-dd85-4b88-bf37-5614470295ed/image.png)

> 👀 ** <span style="color:#7FFFD4;"> 위의 요약 정보에 대한 해석</span>**
>
> * Adj.R-sqaured의 값이 0.542로 낮은 수치를 나타낸다.
>
> * 6개의 변수를 모두 사용하였을 때, Price의 54.2%만을 설명할 수 있다는 의미이다.
> 
> * 이제 다중공선성을 확인하고, 변수를 선택하여 더 좋은 회귀식을 찾아보자.

```python
# 다중공선성 파악을 위해 상관관계 파악하기
cars_x = cars[['EngineSize', 'RPM', 'Weight', 'Length', 'MPGcity', 'MPGhighway']]
cars_x.corr()
```

![](https://velog.velcdn.com/images/dmis/post/b5258622-c8df-43d4-9936-9dbfa34a5e64/image.png)

> 👀 ** <span style="color:#7FFFD4;">상관계수로 알 수 있는 사실</span>**
>
> * MPGcity와 MPGhighway는 0.9 이상의 상관성을 보이므로 다중공선성이 존재함을 알 수 있다.
>
> * 하지만 둘 중 어떠한 변수를 제거해야 하는지는 결정하기 애매하다.
>
> * 이때, 다음과 같이 VIF를 구하여 결정할 수 있다.


```python
from patsy import dmatrices
from statsmodels.stats.outliers_influence import variance_inflation_factor
# dmatrices : 독립변수와 종속변수를 데이터프레임으로 나누어 저장하는 함수
y, x = dmatrices('Price ~ EngineSize + RPM + Weight + Length + MPGcity + MPGhighway', data = cars, return_type = 'dataframe')

# 독립변수끼리의 VIF 값을 계산하여 데이터프레임 만들기
vif_list = []
for i in range(1, len(x.columns)):
    vif_list.append([variance_inflation_factor(x.values, i), x.columns[i]])
vif_df = pd.DataFrame(vif_list, columns = ['vif', 'variable'])
vif_df
```

![](https://velog.velcdn.com/images/dmis/post/86400191-6963-4073-be71-1954390c0c92/image.png)

> 👀 ** <span style="color:#7FFFD4;">VIF를 통해 알 수 있는 사실</span>**
> 
> * MPGcity의 VIF가 MPGhighway의 VIF보다 크므로 MPGcity를 제거해야함을 알 수 있다.

```python
# MPGcity를 삭제하고 다시 다중 선형 회귀분석 진행해보기
model = smf.ols(formula= 'Price ~ EngineSize + RPM + Weight + Length +  MPGhighway', data = cars)
result = model.fit()
result.summary()
```

> 👀 **<span style="color:#7FFFD4;">알 수 있는 사실</span>**
> 
> * 앞서 분석했던 모델보다 $Adj.R^2$과 AIC의 큰 변화는 없는 것을 알 수 있다.
>
> * 다중공선성의 제거는 모델의 성능향상보다는 무분별한 변수 선택으로 데이터의 관리가 어려워지는 현상을 막는 것에 의의가 있다.
> 
> * MPGcity를 제거하자 MPGhighway의 p-value가 현저히 낮아진 것을 알 수 있다.
> 
>   * 이처럼 유의한 변수임에도 다중공선성의 이유로 유의하지 않은 변수처럼 여겨질 수 있다.
> 
>     * 따라서 다중공선성은 꼭 해결해야할 과제이다.

```python
# 코드 확인 필요
# 독립변수 중 유의한 변수를 고르고, 모델의 성능을 최적화시키는 변수 선택법 진행해보기

import time
import itertools

def processSubset(x, y, feature_set) :
    model = sm.OLS(y, x[list(feature_set)]) # modeling
    regr = model.fit() # 모델 학습
    AIC = regr.aic
    return {"model":regr, "AIC":AIC}

# 전진선택법
def forward (x, y, predictors) :
    # 데이터 변수들이 미리 정의된 predictores에 있는지 없는지 확인 및 분류
    remaining_predictors = [p for p in x.columns.difference(['Intercept'])
                            if p not in predictors]
    results = []
    for p in remaining_predictors :
        results.append(processSubset(x=x, y=y, feature_set=predictors+[p]+['Intercept']))

    # 데이터프레임으로 변환
    models = pd.DataFrame(results)
    print(models)
    # AIC가 가장 낮은 것을 선택
    best_model = models.loc[models['AIC'].argmin()] # index
    print('Processed', models.shape[0], 'models on', len(predictors)+1, 'predictors in')
    print('Selected predictors:', best_model['model'].model.exog_names, 'AIC:', best_model[0])

    return best_model

# =====

# 후진제거법
def backward(x, y, predictors) :
    tic = time.time()
    results = []

    # 데이터 변수들이 미리 정의된 predictors 조합 확인
    for combo in itertools.combinations(predictors, len(predictors)+1):
        results.append(processSubset(x=x, y=y, feature_set= list(combo)+['Intercept']))
    models = pd.DataFrame(results)

    # 가장 낮은 AIC를 가진 모델을 선택
    best_model = models.loc[models['AIC'].argmin()] # index
    toc = time.time()

    print('Processed', models.shape[0], 'models on', len(predictors)+1, 'predictors in', (toc - tic))
    print('Selected predictors:', best_model['model'].model.exog_names, 'AIC:', best_model[0])

    return best_model

# =====

# 단계적 선택법
def Stepwise_model(x, y) :
    Stepmodels = pd.DataFrame(columns = ['AIC', 'model'])
    tic = time.time()
    predictors = []
    Smodel_before = processSubset(x, y, predictors+['Intercept'])['AIC']

    for i in range(1, len(x.columns.difference(['Intercept']))+1) :
        forward_result = forward(x=x, y=y, predictors = predictors)
        print('forward')
        Stepmodels.loc[i] = forward_result
        Stepmodels = Stepmodels.loc[i]['model'].model.exog_names
        predictors = [k for k in predictors if k!='Intercept']
        backward_result = backward(x=x, y=y, predictors = predictors)

        if backward_result['AIC'] < forward_result['AIC'] :
            Stepmodels.loc[i] = backward_result
            predictors = Stepmodels.loc[i]['model'].model.exog_names
            Smodel_before = Stepmodels.loc[i]['AIC']
            predictors = [k for k in predictors if k!='Intercept']
            print('backward')

        if Stepmodels.loc[i]['AIC'] > Smodel_before:
            break
        else :
            Smodel_before = Stepmodels.loc[i]['AIC']
    toc = time.time()
    print('Total elapsed time:',(toc-tic), 'seconds.')

    return (Stepmodels['model'][len(Stepmodels['model'])])

Stepwise_best_model = Stepwise_model(x=x, y=y)
```